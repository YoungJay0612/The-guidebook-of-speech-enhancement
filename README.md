# 重点总结
根据综述，提取出一些让人觉着眼前一亮或者总结性非常强的点，仅供参考。

## 损失函数
答：网络架构和学习目标虽然的研究虽然更多然而只是性能提升的一方面，更长时间范围的输入特征以及精心设计的损失函数对模型性能的提升可能更加有益，其中损失函数由于不会在推理过程中引入额外的时延和复杂度而备受关注。  
1：有文章表示，对于时域样本而言MAE相比MSE更关注小信号并具有更强的噪声抑制能力。(这一点在自己的实际应用中确实得到了验证)  
2：目前普遍认为，基于信号近似技术的损失函数相比直接度量掩蔽间的差异对语音增强的性能更有益。将掩码作用于频谱得到估计值，在进行频谱的范数损失计算，也可以是复数谱。
3：尽管有文献提及由于STFT频点分布更类似拉普拉斯分布，因此建议使用L1范数损失，然而大多数工作更喜欢选择L2范数损失。𝐿𝑝范数损失最后也见于对嵌入式特征的距离度量，利用wav2vec、PANN等预训练模型生成嵌入式特征之间的距离作为谱距离损失的正则项。
4：基于信号能量比的损失函数主要用于时域波形，常见的有SDR及其Log形式和SI-SDR损失。
5：。尽管SI-SDR被广泛应用于语音分离当中，但是由于其过多的语音失真导致最近的工作常将其与𝐿𝑝范数谱距离损失联合使用。上述特征常被诟病并不能反映人耳的听感，因此，一些感知指标，如PESQ、ESTOI和CD，一度被引入作为损失函数训练模型。然而这类损失函数仅能提升被优化指标的性能，并没有带来其余指标的提升。

## 基于幅度谱的语音增强
带噪信号通过傅里叶变换基或基于人耳听觉的滤波器组将时域信号分解成二维频域表征，而后提取其幅度域特征，如短时傅里叶变换的幅度谱、对数功率谱等。通过纯净语音和噪声可以计算得到网络的映射目标，常见的是时频掩蔽或与特征提取对应的谱特征。特征提取和分离目标组成输入输出对用于对网络模型进行训练，建立从输入特征到分离目标之间的映射。  

## 基于复数谱的语音增强
### 1：实部和虚部，幅值与相位哪种组合适用于复数谱的语音增强？
答：实部和虚部更合理。文中Fig2.1的语音复数谱图直观的解释了这一问题。语音的各个时频点的相位在时间轴和频率轴上的变化都比较迅速。由于相位缠绕问题，语谱各时频点的相位几乎均匀分布在[−𝜋, 𝜋]之间。相位谱并不存在一个清晰的结构模式，导致网络很难对其学习预测。而复谱的实部和虚部则具有与幅度谱类似的结构模式，使得通过网络估计实部虚部分量从而隐式优化幅度和相位成为可能。

### 2：在复数谱的情况下，实部和虚部的组合方式以及预测方式哪种更合理？
答：Tan在2019年通过实验比较了这几种网络架构的性能区别，不出意外地以上结构都可以实现复数谱映射，而Tan更推荐使用单独的编码器同时编码带噪谱的实部和虚部，而后使用两个解码器
分别估计增强语谱的实部和虚部。该模型后来被成为GCRN，其卓越的性能成为了后续很多语音增强工作的基础。

## 网络因果性的判断-网络层面
1：LSTM和GRU层，torch.nn.LSTM和torch.nn.gru层中的参数bidirectional设为False(默认)，即只使用历史信息；  
2：二维卷积层：需要对torch.nn.Conv2d/Conv1d输入特征的时间维度向历史帧方向补零(时间帧方向卷积核不为1时)；  
3：二维转置卷积：需要对torch.nn.Transpose2d输出结果的时间帧维度进行在未来帧方向截断(时间帧方向卷积不为1时)；  
4：注意力机制模块：需对点积相似度公式中的Softmax函数内加掩蔽项，这样虽然是因果的，但是在推理时内存和运行时间仍会随帧数增加而增长。基于chunk或者类TransformerXL的attention才有可能满足实时处理要求；  
5：标准化层：BatchNormalization应在推理是将模型设为eval()模式；Instance Normalization应将参数track_running_stats设置为True；  
6：池化层：因果模型中不应对时间帧维度进行池化。  

# The-guidebook-of-speech-enhancement

这是一个自娱自乐的项目。

Sorry, this repostitory is just for fun, and does not support English or other non-Chinese languages at this time.

自2018年Prof. Wang的[Supervised Speech Separation Based on Deep Learning: An Overview](https://ieeexplore.ieee.org/document/8369155)之后，
似乎总缺少关于语音增强领域的综述。然而不论是学术界还是业界，近年来语音增强领域的发展都是迅速的。
因此，面对他人问及有关深度学习语音增强技术的资料时，我总不能找到一份于今日而言仍是满意的文献或博客。

另一方面，近年来无疑语音增强的涉入者日趋增多，开源社区和研究文献愈加丰富，远胜于当年。虽然我对开源文化并不积极，抑或说对降低门槛之类的言论仍持否定。
但不得不承认，这些工作确实一定程度上加速了社区的发展。然而，其中一些常不慎落入陷阱之中出现失误，这些失误若不能鉴别可能会受其误导(包括但不限于有误的结论、不公平的消融实验以及审稿人要求与有误的文章进行对比等)。

最后，适当结合论文、代码和社区可能对初学者更有帮助，在另一个项目[awesome-speech-enhancement](https://github.com/WenzheLiu-Speech/awesome-speech-enhancement)中我已经试图将一些代表性文章、对应代码和作者的github账号结合展示，
很高兴这个项目得到了社区中许多同伴的支持和帮助。因此，这点的沿用似乎是可以的，但是仅仅以这种形式却无法解决第二点问题。

恰逢一段闲暇时，便开了这样一个项目以抛砖引玉。当然这个项目既不敢称为综述(也确实不是综述)，亦不能称之为书(却也不足以成书)，甚至不知以我之拖延能否坚持写完。只希望项目完成之时供初学者能做个参考，
但凡有一星半点对他人有益，即可谓得其所哉了。

一则此项目望供初学者初窥门径，二则笔者也尚在探究之中，故命名以 语音增强初探

本项目可通过[该链接🔗](https://wenzheliu-speech.github.io/The-guidebook-of-speech-enhancement/se_development.pdf)访问查看。


* 本项目因并未完成且涉及社区开源代码，暂不支持任何形式的转载；
* 本项目[**请勿**]()issue、fork和PR，如有问题或希望参与其中可[邮件](liuwenzhe@mail.ioa.ac.cn)沟通；
* 本项目参考文献引用尚不完全、内容尚待更新、图文尚需校正；之所以现在设为public是因为无法在项目private时通过网址访问；
* 本项目暂不考虑英文版；
* 本项目初步打算完成基于深度学习的单通道语音增强算法部分(Part 1)，后续可能会拓展去混响、AEC、AGC和阵列部分作为其他Part；
* Latex模板修改自Springer

行笔匆匆，疏漏难免。如有错讹，烦请见谅。


